{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DEEPAK REDDY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[[   0    0    0 ...    0    0    0]\n",
      " [   6    1  103 ...    0    0    0]\n",
      " [ 102   90 1161 ...    0    0    0]\n",
      " [  18  256  108 ...    0    0    0]\n",
      " [   4  828 4435 ...    0    0    0]]\n",
      "WARNING:tensorflow:From c:\\Users\\DEEPAK REDDY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\DEEPAK REDDY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 933)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 933, 100)          3013500   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 933, 200)          160800    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 933, 30135)        6057135   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9231435 (35.22 MB)\n",
      "Trainable params: 9231435 (35.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\DEEPAK REDDY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "  3/137 [..............................] - ETA: 6:54:29 - loss: 10.3007"
     ]
    }
   ],
   "source": [
    "# Cell 1: Data Collection and Preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a CSV file named 'books.csv' with columns 'title', 'description'\n",
    "df = pd.read_csv('books.csv')\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle missing values in the 'description' column\n",
    "train_df['description'].fillna('', inplace=True)\n",
    "test_df['description'].fillna('', inplace=True)\n",
    "\n",
    "# Tokenize book descriptions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['description'])\n",
    "\n",
    "# Convert descriptions into sequences and pad them\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['description'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['description'])\n",
    "\n",
    "max_sequence_length = max(len(seq) for seq in train_sequences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Display the tokenized sequences\n",
    "print(X_train[:5])\n",
    "# ... (Your existing code)\n",
    "# Cell 2: Model Architecture (Adjusted)\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "# Adjusted model architecture\n",
    "input_layer = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(total_words, 100, input_length=max_sequence_length)(input_layer)\n",
    "lstm_layer = Bidirectional(LSTM(100, return_sequences=True))(embedding_layer)  # Using Bidirectional LSTM for enhanced learning\n",
    "output_layer = Dense(total_words, activation='softmax')(lstm_layer)  # Using 'softmax' for output layer as it's a classification task\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')  # Changed loss function for a classification task\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Cell 3: Model Training (Use more epochs)\n",
    "model.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Cell 4: Model Evaluation\n",
    "loss = model.evaluate(X_test, X_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Cell 5: User Input and Recommendation (Unchanged)\n",
    "user_query = input(\"Enter a book description: \")\n",
    "print(user_query)\n",
    "# ... (remaining code for recommendation)\n",
    "\n",
    "\n",
    "# Create a dictionary to store book embeddings\n",
    "book_embeddings = {}\n",
    "\n",
    "# Store embeddings in the dictionary\n",
    "for index, row in train_df.iterrows():\n",
    "    description = row['description']\n",
    "    sequence = tokenizer.texts_to_sequences([description])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
    "    embedding = model.predict(padded_sequence)[0]\n",
    "    book_embeddings[row['title']] = {'embedding': embedding, 'description': description}\n",
    "\n",
    "# Tokenize and pad user input\n",
    "user_sequence = tokenizer.texts_to_sequences([user_query])\n",
    "user_padded = pad_sequences(user_sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Get the user's input embedding\n",
    "user_embedding = model.predict(user_padded)[0]\n",
    "\n",
    "# Calculate cosine similarity for each book in the dictionary\n",
    "similarities = {}\n",
    "for title, data in book_embeddings.items():\n",
    "    book_embedding = data['embedding']\n",
    "    similarity = np.dot(book_embedding, user_embedding) / (np.linalg.norm(book_embedding) * np.linalg.norm(user_embedding))\n",
    "    similarities[title] = similarity\n",
    "\n",
    "# Sort by similarity and get top 10 recommendations\n",
    "# Sort by similarity and get top 10 recommendations\n",
    "top_recommendations = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 Recommended Books:\")\n",
    "for title, similarity in top_recommendations:\n",
    "    print(f\"{title}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Data Collection and Preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming you have a CSV file named 'books.csv' with columns 'title', 'description'\n",
    "df = pd.read_csv('books.csv')\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle missing values in the 'description' column\n",
    "train_df['description'].fillna('', inplace=True)\n",
    "test_df['description'].fillna('', inplace=True)\n",
    "\n",
    "# Tokenize book descriptions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['description'])\n",
    "\n",
    "# Convert descriptions into sequences and pad them\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_df['description']), padding='post')\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_df['description']), padding='post')\n",
    "\n",
    "# Display the tokenized sequences with word embeddings\n",
    "print(X_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have a CSV file named 'books.csv' with columns 'title', 'description'\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv('books.csv')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        words = text.split()  # Change from text_to_word_sequence to split\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "df['description'] = df['description'].apply(preprocess_text)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['description'])\n",
    "sequences = tokenizer.texts_to_sequences(df['description'])\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, _, _, _ = train_test_split(X, X, test_size=0.3, random_state=42)\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# Model Architecture\n",
    "embedding_size = 250\n",
    "book_model = Sequential()\n",
    "book_model.add(Embedding(total_words, embedding_size, input_length=max_sequence_length))\n",
    "book_model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "book_model.add(TimeDistributed(Dense(max_sequence_length, activation='linear')))  # Use TimeDistributed layer here\n",
    "book_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Model Training\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "book_model.fit(X_train, X_train, epochs=1, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Book Recommendation Function\n",
    "def recommend_books(user_query, book_model, tokenizer, df, top_n=5):\n",
    "    user_sequence = tokenizer.texts_to_sequences([user_query])\n",
    "    user_padded = pad_sequences(user_sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    book_embedding = book_model.predict(X_train)  # Use X_train here\n",
    "    user_embedding = book_model.predict(user_padded)\n",
    "\n",
    "    similarity_scores = cosine_similarity(user_embedding, book_embedding)\n",
    "\n",
    "    indices = similarity_scores.argsort(axis=1)[0, ::-1][:top_n]\n",
    "    recommended_books = df.loc[indices, ['title', 'description']]\n",
    "\n",
    "    return recommended_books\n",
    "\n",
    "# User Interaction\n",
    "user_query = input(\"Enter a book description: \")\n",
    "recommended_books = recommend_books(user_query, book_model, tokenizer, df)\n",
    "print(\"Recommended Books:\")\n",
    "print(recommended_books)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Step 2: Load the Dataset\n",
    "dataset_path = \"books.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Step 3: Text Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['description'].fillna(''))\n",
    "\n",
    "# Step 4: Calculate Similarity Scores\n",
    "similarity_scores = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Step 5: Recommendation Function\n",
    "def get_recommendations(title, similarity_matrix, dataframe):\n",
    "    idx = dataframe.index[dataframe['title'] == title].tolist()[0]\n",
    "    sim_scores = list(enumerate(similarity_matrix[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:11]  # Top 10 recommendations (excluding the input book)\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "    return dataframe['title'].iloc[book_indices]\n",
    "\n",
    "# Step 6: Test the Recommendation System\n",
    "book_title = \"The Catcher in the Rye\"  # Replace with your desired title\n",
    "recommendations = get_recommendations(book_title, similarity_scores, df)\n",
    "print(\"Recommendations for {}: \\n{}\".format(book_title, recommendations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user input : romeo and juliet hate story.\n",
      "Recommendations based on user description: \n",
      "4597              Falling for You\n",
      "5469     Mockingbird Wish Me Luck\n",
      "330     The Chaneysville Incident\n",
      "572           In Watermelon Sugar\n",
      "1507                  Plum Lovin'\n",
      "3                  Rage of angels\n",
      "882             Henry IV Part Two\n",
      "879        The Merchant of Venice\n",
      "5922                   Harm's Way\n",
      "4022          Written on the Body\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Step 2: Load the Dataset\n",
    "dataset_path = \"books.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Step 3: Text Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['description'].fillna(''))\n",
    "df[\"description\"].head()\n",
    "df[df[\"description\"].isnull()].head()\n",
    "df['description'] = df['description'].fillna('') \n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = tfidf.fit_transform(df['description'])\n",
    "tfidf_matrix.shape\n",
    "# Step 4: Calculate Similarity Scores\n",
    "similarity_scores = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Step 5: Recommendation Function for User Description\n",
    "def get_recommendations_for_user(description, similarity_matrix, dataframe):\n",
    "    # Vectorize the user description\n",
    "    user_vector = tfidf_vectorizer.transform([description])\n",
    "\n",
    "    # Calculate similarity scores between user description and book summaries\n",
    "    user_similarity_scores = linear_kernel(user_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get indices of books with highest similarity scores\n",
    "    book_indices = user_similarity_scores.argsort()[:-11:-1]  # Top 10 recommendations\n",
    "\n",
    "    return dataframe['title'].iloc[book_indices]\n",
    "\n",
    "# Step 6: Test the Recommendation System with User Description\n",
    "user_description = \"romeo and juliet hate story.\"\n",
    "print(\"user input : {}\".format(user_description))\n",
    "recommendations = get_recommendations_for_user(user_description, similarity_scores, df)\n",
    "print(\"Recommendations based on user description: \\n{}\".format(recommendations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the Dataset\n",
    "df = pd.read_csv('books.csv')\n",
    "\n",
    "# Fill NaN values in the 'description' column\n",
    "df['description'] = df['description'].fillna('')\n",
    "\n",
    "# Text Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = tfidf.fit_transform(df['description'])\n",
    "\n",
    "# Calculate Cosine Similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Recommendation Function for User Description\n",
    "def content_based_recommender(description, cosine_sim, dataframe):\n",
    "    # Vectorize the user description\n",
    "    user_vector = tfidf.transform([description])\n",
    "\n",
    "    # Calculate similarity scores between user description and book summaries\n",
    "    user_similarity_scores = cosine_similarity(user_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get indices of books with highest similarity scores\n",
    "    book_indices = user_similarity_scores.argsort()[:-11:-1]  # Top 10 recommendations\n",
    "\n",
    "    return dataframe['title'].iloc[book_indices]\n",
    "\n",
    "# Test the Recommendation System with User Description\n",
    "user_description = \"I enjoy thrilling stories with a touch of suspense.\"\n",
    "recommendations = content_based_recommender(user_description, cosine_sim, df)\n",
    "print(\"Recommendations based on user description: \\n{}\".format(recommendations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
