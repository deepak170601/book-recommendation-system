{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Data Collection\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a CSV file named 'books.csv' with columns 'title', 'description'\n",
    "df = pd.read_csv('books.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# Cell 2: Data Preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Handle missing values in the 'description' column\n",
    "df['description'].fillna('', inplace=True)\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize book descriptions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['description'])\n",
    "\n",
    "# Convert descriptions into sequences and pad them\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['description'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['description'])\n",
    "\n",
    "max_sequence_length = max(len(seq) for seq in train_sequences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Display the tokenized sequences\n",
    "print(X_train[:5])\n",
    "\n",
    "X_array = np.array(X_train)\n",
    "\n",
    "# Cell 3: Model Architecture\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(max_sequence_length,))\n",
    "# Embedding layer\n",
    "embedding_layer = Embedding(total_words, 100, input_length=max_sequence_length)(input_layer)\n",
    "# Bidirectional LSTM layer for better learning\n",
    "lstm_layer = Bidirectional(LSTM(100, return_sequences=True))(embedding_layer)\n",
    "# Dense layer for reconstruction\n",
    "output_layer = Dense(total_words, activation='softmax')(lstm_layer)  # Adjusted activation function\n",
    "\n",
    "# Build the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')  # Adjusted loss function\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Cell 4: Model Training\n",
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_array, X_array, epochs=5, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Cell 5: User Input and Recommendation\n",
    "user_query = input(\"Enter a book description: \")\n",
    "print(user_query)\n",
    "\n",
    "# Tokenize and pad user input\n",
    "user_sequence = tokenizer.texts_to_sequences([user_query])\n",
    "user_padded = pad_sequences(user_sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Get reconstructed sequence\n",
    "reconstructed_sequence = model.predict(user_padded)\n",
    "\n",
    "# Store reconstructed sequence in the original DataFrame\n",
    "df['reconstructed_sequence'] = model.predict(X_array).tolist()\n",
    "\n",
    "# Calculate similarity and recommend books\n",
    "df['similarity'] = df['reconstructed_sequence'].apply(\n",
    "    lambda x: np.linalg.norm(np.array(x) - np.array(reconstructed_sequence[0]))\n",
    ")\n",
    "\n",
    "# Sort by similarity to user input\n",
    "recommended_books = df.sort_values(by='similarity').head(12)[['title' , 'similarity']]\n",
    "\n",
    "print(\"Recommended Books:\")\n",
    "print(recommended_books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = input(\"Enter a book description: \")\n",
    "\n",
    "# Tokenize and pad user input\n",
    "user_sequence = tokenizer.texts_to_sequences([user_query])\n",
    "user_padded = pad_sequences(user_sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Get reconstructed sequence\n",
    "reconstructed_sequence = model.predict(user_padded)\n",
    "\n",
    "# Store reconstructed sequence in the original DataFrame\n",
    "df['reconstructed_sequence'] = model.predict(X_array).tolist()\n",
    "\n",
    "# Calculate similarity and recommend books\n",
    "df['similarity'] = df['reconstructed_sequence'].apply(\n",
    "    lambda x: np.linalg.norm(np.array(x) - np.array(reconstructed_sequence[0]))\n",
    ")\n",
    "\n",
    "# Sort by similarity to user input\n",
    "recommended_books = df.sort_values(by='similarity').head(12)[['title' , 'similarity']]\n",
    "\n",
    "print(\"Recommended Books:\")\n",
    "print(recommended_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Data Collection\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a CSV file named 'books.csv' with columns 'title', 'description'\n",
    "df = pd.read_csv('books.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# Cell 2: Data Preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Handle missing values in the 'description' column\n",
    "df['description'].fillna('', inplace=True)\n",
    "\n",
    "# Tokenize book descriptions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['description'])\n",
    "\n",
    "# Convert descriptions into sequences and pad them\n",
    "sequences = tokenizer.texts_to_sequences(df['description'])\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Display the tokenized sequences\n",
    "print(X[:5])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_array = np.array(X)\n",
    "\n",
    "# Cell 3: Model Architecture\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(max_sequence_length,))\n",
    "# Embedding layer\n",
    "embedding_layer = Embedding(total_words, 100, input_length=max_sequence_length)(input_layer)\n",
    "# LSTM layer\n",
    "lstm_layer = LSTM(50)(embedding_layer)\n",
    "# Dense layer for reconstruction\n",
    "output_layer = Dense(max_sequence_length, activation='linear')(lstm_layer)\n",
    "\n",
    "# Build the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "# ...\n",
    "# Cell 4: Model Training\n",
    "model.fit(X_array, X_array, epochs=1, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Cell 5: Recommendation\n",
    "# Take user input from console\n",
    "user_query = input(\"Enter a book description: \")\n",
    "\n",
    "# Tokenize and pad user input\n",
    "user_sequence = tokenizer.texts_to_sequences([user_query])\n",
    "user_padded = pad_sequences(user_sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Get reconstructed sequence\n",
    "reconstructed_sequence = model.predict(user_padded)\n",
    "\n",
    "# Store reconstructed sequence in the original DataFrame\n",
    "df['reconstructed_sequence'] = model.predict(X_array).tolist()\n",
    "\n",
    "# Calculate similarity and recommend books\n",
    "df['similarity'] = df['reconstructed_sequence'].apply(\n",
    "    lambda x: np.linalg.norm(np.array(x) - np.array(reconstructed_sequence[0]))\n",
    ")\n",
    "\n",
    "# Sort by similarity to user input\n",
    "recommended_books = df.sort_values(by='similarity').head(12)[['title' , 'similarity']]\n",
    "\n",
    "print(\"Recommended Books:\")\n",
    "print(recommended_books)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    4    51    11 ...     0     0     0]\n",
      " [    4    21 17955 ...     0     0     0]\n",
      " [  121    59     2 ...     0     0     0]\n",
      " [    4  1923  4849 ...     0     0     0]\n",
      " [12965    62    13 ...     0     0     0]]\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 933)]             0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 933, 100)          3373500   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 933, 50)           30200     \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 100)               40400     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 933)               94233     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3538333 (13.50 MB)\n",
      "Trainable params: 3538333 (13.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "171/171 [==============================] - 207s 1s/step - loss: 2269170.7500 - val_loss: 3963157.7500\n",
      "Epoch 2/5\n",
      "171/171 [==============================] - 193s 1s/step - loss: 2264124.2500 - val_loss: 3957168.7500\n",
      "Epoch 3/5\n",
      "171/171 [==============================] - 185s 1s/step - loss: 2259594.0000 - val_loss: 3951485.2500\n",
      "Epoch 4/5\n",
      "171/171 [==============================] - 181s 1s/step - loss: 2255274.0000 - val_loss: 3946000.5000\n",
      "Epoch 5/5\n",
      "171/171 [==============================] - 179s 1s/step - loss: 2251121.5000 - val_loss: 3940723.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x25020e18150>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Data Collection\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a CSV file named 'books.csv' with columns 'title', 'description'\n",
    "df = pd.read_csv('books.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "\n",
    "# Cell 2: Data Preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Handle missing values in the 'description' column\n",
    "df['description'].fillna('', inplace=True)\n",
    "\n",
    "# Tokenize book descriptions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['description'])\n",
    "\n",
    "# Convert descriptions into sequences and pad them\n",
    "sequences = tokenizer.texts_to_sequences(df['description'])\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Display the tokenized sequences\n",
    "print(X[:5])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_array = np.array(X)\n",
    "\n",
    "# Cell 3: Model Architecture\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(max_sequence_length,))\n",
    "# Embedding layer\n",
    "embedding_layer = Embedding(total_words, 100, input_length=max_sequence_length)(input_layer)\n",
    "# LSTM layer\n",
    "lstm_layer = LSTM(50, return_sequences=True)(embedding_layer)\n",
    "# Bidirectional LSTM layer for better representation\n",
    "bidirectional_lstm = Bidirectional(LSTM(50))(lstm_layer)\n",
    "# Dense layer for reconstruction\n",
    "output_layer = Dense(max_sequence_length, activation='linear')(bidirectional_lstm)\n",
    "\n",
    "# Build the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Cell 4: Model Training\n",
    "model.fit(X_array, X_array, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Recommendation\n",
    "# Take user input from console\n",
    "user_query = input(\"Enter a book description: \")\n",
    "\n",
    "# Tokenize and pad user input\n",
    "user_sequence = tokenizer.texts_to_sequences([user_query])\n",
    "user_padded = pad_sequences(user_sequence, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Get reconstructed sequence\n",
    "reconstructed_sequence = model.predict(user_padded)\n",
    "\n",
    "# Store reconstructed sequence in the original DataFrame\n",
    "df['reconstructed_sequence'] = model.predict(X_array).tolist()\n",
    "\n",
    "# Calculate similarity and recommend books\n",
    "df['similarity'] = df['reconstructed_sequence'].apply(\n",
    "    lambda x: np.linalg.norm(np.array(x) - np.array(reconstructed_sequence[0]))\n",
    ")\n",
    "\n",
    "\n",
    "# Sort by similarity to user input in descending order\n",
    "recommended_books = df.sort_values(by='similarity', ascending=False).head(12)[['title', 'similarity']]\n",
    "\n",
    "print(\"Recommended Books:\")\n",
    "print(recommended_books)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
